# SqueezeNet_Fashion_MNIST
Implementation of a lightweight "SqueezeNet" CNN, modified with depthwise-separable convolution layers for further model size reduction

This notebook demonstrates that both a base SqueezeNet model - as described by "SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE" - as well as the SqueezeNet model augmented with depthwise-separable layers - as described by "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications" - able to achieve >80% accuracy on the 10,000 test images from the Fashion MNIST dataset. Specifically, the base SqueezeNet achieved an accuracy of 85% after 15 epochs with a learning rate of 0.0003 and a batch size of 300, and the SqueezeNet using DWS layers achieved an accuracy of 84% after 3 epochs with a learning rate of 0.01 and a batch size of 100. The key takeaways from these experiments are that: 1. Base SqueezeNet architecture and Fire Layers, are able to achieve decent performance in a multiclass image classification setting and showcase potential for improved performance based on increasing training time alone; and 2. Implementing DWS layers within the SqueezeNet's architecture and Fire Layers is able to drastically decrease model size (from 1249026 parameters to 761014 - more than 39% under this particular architecture) while achieving comparable performance with similar indications of potential improvement based on trends of model loss (both models' printed loss per 1/10 batch shows loss has not converged at the end of training). Therefore, for further increase in performance for both models, models should be trained to convergence instead of simple a set number of epochs. Hyperparameter tuning can additionally be done on the learning rate, batch_size, and macroarchitecture defined by conv_layer_params and fire_layer_params. In terms of macroarchitecture, the models can be further pared down by adding in maxpool layers as shown in Figure 2 of the SqueezeNet paper (ommitted for this exercise) and introducing network pruning; implementing bypass connections between multiple layers to improve accuracy (also suggested by the SqueezeNet paper); and otherwise reformatted in terms of adding or dropping layers based on external resource constraints. As a final footnote: it was observed that the basse SqueezeNet model trained on input data faster than the DWS SqueezeNet despite having more parameters. It was not determined what the exact cause of this was, but it is likely due to the overhead of creating multiple new tensors,and the additional steps of computation (including a loop) in the place of a single nn.Conv2d layer, which should slow down computation both in forward and back-propagation during training.
